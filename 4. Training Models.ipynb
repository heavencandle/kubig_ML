{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training model = setting its parameters so that the model best fits the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression model prediction  \n",
    "    - Î¸_0: bias term\n",
    "    - x_0: always equal to zero  \n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_11.png\" width=\"25%\" height=\"25%\" align=\"left\"><br>  \n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_12.png\" width=\"15%\" height=\"15%\" align =\"left\"><br>  \n",
    "<br>\n",
    "2. MSE cost function for a Linear Regression model<br>\n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_18.png\" width=\"25%\" height=\"25%\" align =\"left\"><br>\n",
    "<br><br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The Normal Equation  \n",
    "> usage: **good for less features, many instances**  \n",
    "\n",
    "    - normal equation: closed-form equation; a mathematical equation that gives the result directly  \n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_19.png\" width=\"15%\" height=\"15%\" align = \"left\"><br><br>\n",
    "    - Positive:   \n",
    "        1) equation is linear with regards to the **# of instances** in the training set; O(m)  \n",
    "        -> handles large training sets efficiently  \n",
    "        2) Once trained, predictions are very fast. computational complexity is linear with regards to both # of instances and # of features.(i.e. instance # twice -> time twice)       \n",
    "    - Negative:   \n",
    "        1) gest very slow when the number of **features** grow large; O(n^3)  \n",
    "        inversing cost is high. X.T * X -> (n, n) shape. n: # of features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_best: [[3.80376336]\n",
      " [2.96988319]]\n",
      "y_predict: [[3.80376336]\n",
      " [9.74352973]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3-1. Normal Equation w/o scikit-linear\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = 2 * np.random.rand(100, 1) # \n",
    "y = 4 + 3 * X + np.random.randn(100, 1) #\n",
    "\n",
    "# training: use normal equation\n",
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance; bias\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) \n",
    "theta_best #  similar with answer theta [4, 3]\n",
    "print(f'theta_best: {theta_best}')\n",
    "\n",
    "# sample prediction\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2,1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "print(f'y_predict: {y_predict}')\n",
    "\n",
    "# plotting\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3.9102045]), array([[3.16203789]]))\n",
      "[[ 3.9102045 ]\n",
      " [10.23428029]]\n"
     ]
    }
   ],
   "source": [
    "# 3-2. Normal Equation w scikit-linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#train\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(f'{lin_reg.intercept_, lin_reg.coef_}')\n",
    "   \n",
    "#predict\n",
    "X_new = np.array([[0], [2]]) # do not need to add bias x_0\n",
    "print(lin_reg.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **when using Gradient Descent, ensure that all features have a similar scale(e.g. *StandardScaler class*). Or it'll take long to diverge**\n",
    "- learning rate\n",
    "    - too small: have to go through many iterations to converge(takes long time)\n",
    "    - too high: jump across the valley and end up on the other side, make algorithm diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batch Gradient Descent\n",
    "    - Batch Gradient Descent: uses the whole batch of training data at every step; calculates over the full training set X at each Gradient Descent step\n",
    "    - Partial derivatives of the cost function  \n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_29.png\" width= '30%' height= '30%' align='left'><br><br><br><br><br><br>\n",
    "    - Gradient vector of the cost function(compute partial derivatives in one go)<br>\n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_28.png\" width= '30%' height= '30%' align='left'><br><br><br><br>\n",
    "    - Gradient Descent step\n",
    "        - eta: learning rate<br>\n",
    "    <img src = \"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/eq_30.png\" width= '30%' height= '30%' align='left'><br><br><br>\n",
    "    - Hyeperparameters\n",
    "        1. Leraning rate\n",
    "            - **to find a good learning rate, ue grid search.** However, we may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "        2. Iterations\n",
    "            - **set a very large number of iterations but interrupt the algorithm when the gradient vector becomes tiny**; when its norm becoes smaller than a tiny number Ïµ(tolerance)\n",
    "    - *Convergence rate (suppose the cost function is convex and its slope does not change abruptly)*\n",
    "        - Batch Gradient Descent with a fixed learning rate has a convergence rate of O(1/Ïµ)\n",
    "        - if Ïµ divided by 10, the algorithm has to run about 10 times more iterations\n",
    "    \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.80376336],\n",
       "       [2.96988319]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "'''\n",
    "\n",
    "eta = 0.1 #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) #random initialization\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stochastic Gradient Descent\n",
    "    - Stochastic Gradient Descent: picks a random instance in the training set at every step and computes the gradients based only on that single instance.\n",
    "    - If you want to be sure that algorithm goes through every instance at each epoch, shuffle the training set then go through it instance by instance then shuffle again. But it generally converges more slowly\n",
    "        - Pos: **1) Makes the alg much faster**; has very little data to manipulate at each iteration. **2) Has a better chance of finding global minimum than BGD** when the cost function is very irregular\n",
    "        - Neg: **final parameter values are good but not normal.** Because it's much less regular than Batch Gradient Descent; cost function bounces up and down, decreasing only on average. Ends up very close to the minumum but bounce around, never settling down.\n",
    "            - solution: simulated annealing(gradually reduce the larning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent w/ learning schedule w/o scikit-learn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50\n",
    "m = 100\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "# select m instances every epoch\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_idx =  np.random.randint(m)\n",
    "        xi = X_b[random_idx:random_idx+1]\n",
    "        yi = y[random_idx:random_idx+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i) # epoch * m + i = accumulated instance numbers\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "        plt.plot(theta[0], theta[1], \"b.\")\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "         print(f'eta: {eta:.5f}, theta: {theta[0]}, {theta[1]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.77503707]), array([2.99175798]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent w/ scikit-learn\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=50, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel()) #ravel: flattening\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_\n",
    "#intercept: The intercept term\n",
    "#coef: Weights assigned to the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mini-batch Gradient Descent\n",
    "    - Mini-batch Gradient Descent: Instead of full training set(Batch GD) or just one instance(Stochastic GD), based on small random sets of instances called mini-batches\n",
    "    - Pos: be able to get a performance boost from hardware optimization of matrix operations. alg's progress in parameter space is less erratic than SGD especially with fairly large mini-batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Conclusion  \n",
    "\n",
    "|Algorithm| Large m(instance) | Out-of-core support | Large n(feature) | Hyperparams | Scaling required | Scikit-Learn|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| `Normal Equation` | Fast | No | Slow | 0 | No | n/a |\n",
    "| `SVD` | Fast | No | Slow | 0 | No | LinearRegression |\n",
    "| `Batch GD` | Slow | No | Fast | 2 | Yes | SGDRegressor|\n",
    "| `Stochastic GD` | Fast | Yes | Fast | â¥2 | Yes | SGDRegressor|\n",
    "| `Mini-batch GD` | Fast | Yes | Fast | â¥2 | Yes | SGDRegressor|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- be able to use linear model to fit nonlinear data **by adding powers of each feature a new features**\n",
    "- PolynomialFeatures(degree=d) transforms an array containing n features into an array containing \n",
    "${\\frac{(n+d)!}{d!n!}}$\n",
    "    -> be careful w/ feature #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.09603788]), array([[0.99031658, 0.49535408]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]\n",
    "X_poly[0]\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4. Regualarized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ridge Regression\n",
    "2. Lasso Regression\n",
    "3. Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-5. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-6. Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **a modelâs generalization error can be expressed as the sum of three very different errors**    \n",
    "\n",
    "\n",
    "1. Bias: due to wrong assumptions such as assuming that the data is linear when it is actually quadratic. a high-bias model is most likely to underfit the training data.\n",
    "2. Variance: due to the modelâs excessive sensitivity to small variations in the training data. A model with many degrees of freedom(such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data.\n",
    "3. Irreducible error: due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).\n",
    "\n",
    "\n",
    "- Bias-variance tradeoff\n",
    "    - Increasing a modelâs complexity: typically increase its variance and reduce its bias. \n",
    "    - Reducing a modelâs complexity: increases its bias and reduces its variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Measuring the model's generalization performance: **1) cross-validation, 2) learning curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "batch gradient, stochastic gradient, mini-batch gradient\n",
    "#### 2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
    "batch gradient, stochastic gradient, mini-batch gradient  \n",
    "would take long time to converge  \n",
    "scale features uing standardizing  \n",
    "#### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "#### 4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "yes\n",
    "#### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "overfitting  \n",
    "increase the number of train set or reduce the complexity of the model\n",
    "#### 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "no, \n",
    "it is natural for validation error rate to go up at the beginning\n",
    "#### 7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "mini-batch gradient descent\n",
    "#### 8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "#### 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter Î± or reduce it?\n",
    "#### 10. Why would you want to use: Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?Lasso instead of Ridge Regression? Elastic Net instead of Lasso?\n",
    "#### 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "#### 12. Implement Batch Gradient Descent with early stopping or Softmax Regression (without using ScikitLearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
